{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils import generator, read_batch\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import LongTensor, FloatTensor\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import read_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file_idxes = 'data/file_idxes.npy'\n",
    "p_tokenizer = 'data/tokenizer.pickle'\n",
    "p_label2idx = 'data/label2idx.pickle'\n",
    "p_label_counter = 'data/label_counter'\n",
    "p_word_vectors = './data/word_vectors.npy'\n",
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idxes = np.load(p_file_idxes)\n",
    "word_vectors = np.load(p_word_vectors)\n",
    "with open(p_label2idx, 'rb') as f:\n",
    "    label2idx = pickle.load(f)\n",
    "with open(p_label_counter, 'rb') as f:\n",
    "    label_counter = pickle.load(f)\n",
    "with open(p_tokenizer, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 120  313]\n",
      " [   0  394]\n",
      " [   0  529]\n",
      " [   0 1275]] [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[293]\n",
      " [626]\n",
      " [109]] [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, lab) in enumerate(generator(file_idxes, data_path, label2idx)):\n",
    "    print(seq, lab)\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextsDataset(Dataset):\n",
    "    def __init__(self, file_idxes, data_path, label2idx):\n",
    "        self.file_idxes = file_idxes\n",
    "        self.data_path = data_path\n",
    "        self.label2idx = label2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_idxes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x, batch_y = read_batch(self.data_path, self.file_idxes[idx], self.label2idx)\n",
    "        return LongTensor(batch_x), FloatTensor(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self, kernel_size, n_filters, embedding_dim, word_vectors, n_labels, freeze_embeds=True):\n",
    "        super(ConvClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_vectors, freeze_embeds)\n",
    "        self.conv = nn.Conv2d(1, n_filters, [kernel_size, embedding_dim])\n",
    "        self.linear = nn.Linear(n_filters, n_labels)\n",
    "        \n",
    "    def forward(self, batch_in):\n",
    "        batch_in = torch.unsqueeze(batch_in, 1)\n",
    "        print(batch_in.size())\n",
    "        x = self.embedding(batch_in)\n",
    "        print(x.size())\n",
    "        x = self.conv(x)\n",
    "        print(x.size())\n",
    "        x = torch.squeeze(x, -1)\n",
    "        print(x.size())\n",
    "        x = nn.MaxPool1d(x.size()[2])(x)\n",
    "        print(x.size())\n",
    "        x = torch.squeeze(x, 2)\n",
    "        print(x.size())\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 2])\n",
      "torch.Size([4, 1, 2, 200])\n",
      "torch.Size([4, 10, 1, 1])\n",
      "torch.Size([4, 10, 1])\n",
      "torch.Size([4, 10, 1])\n",
      "torch.Size([4, 10])\n",
      "0.7032218\n"
     ]
    }
   ],
   "source": [
    "dataset = TextsDataset(file_idxes, data_path, label2idx)\n",
    "dataloader = DataLoader(dataset, num_workers=1, collate_fn=lambda l: l[0])\n",
    "clf = ConvClassifier(2, 10, 200, FloatTensor(word_vectors), len(label2idx))\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, clf.parameters()))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for batch_x, batch_y in dataloader:\n",
    "    logits = clf(batch_x)\n",
    "    loss = criterion(logits, batch_y)\n",
    "    print(loss.data.numpy())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
